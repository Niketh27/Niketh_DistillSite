[
  {
    "path": "posts/2021-08-17-xrayprojectpost/",
    "title": "Diagnosing Pediatric Pneumonia by Classification of X-Ray Images",
    "description": "In this project I used the Keras package with the goal of building a CNN classification model for a dataset of X-Ray images to distinguish between diseased and normal X-Rays with an accuracy and sensitivity greater than 0.9.",
    "author": [
      {
        "name": "Niketh Gamage",
        "url": {}
      }
    ],
    "date": "2021-08-18",
    "categories": [],
    "contents": "\r\n\r\nIn this project, we will look at classifying X-Ray images for the presence of signs of Pediatric Pneumonia using a Convolutional Neural Network. The dataset, which is linked here, contains about 5800 X-Ray Images obtained from Guangzhou Women and Children’s Medical Center, Guangzhou from patients age 1-5. All these images are classified as ‘Normal’ or ‘Pneumonia’ by a team of physicians before the dataset was released.\r\nWe will be using about 5200 of these images for training and about 600 images for testing. My 2 goals for this model were to have both an overall accuracy and a sensitivity of above 0.9.\r\nI have presented below 2 example images for each class.\r\nNormal X-Ray: \r\nPneumonia X-Ray: \r\n\r\n\r\nlibrary(caret) #for constructing confusion matrix\r\nlibrary(tidyverse) \r\nlibrary(tensorflow) #modelling\r\nlibrary(keras) #modelling\r\n\r\n\r\n\r\nThe keras package is a very high level neural networks API that simplifies a lot of the model building process in building deep learning models. This is designed in python but the package creates a conda environment in RStudio where the R code is translated into Python and run.\r\nFirst we define the paths to the folders containing the relevant images.\r\n\r\n\r\ntrain_dir  = 'chest_xray/train/'\r\n\r\n\r\nvalidation_dir = 'chest_xray/val/'\r\n\r\n\r\ntest_dir       = 'chest_xray/test/'\r\n\r\n\r\n\r\nThen we define some variables that we will be using in the model building process.\r\nImage size is a very important parameter to decide upon in the model building process. The larger your images are, the more information the network has to process, which allows it to come to more accurate conclusions. It also means that training the model takes a lot longer and you would require more and more computational power. The sweet spot for this model was around 150x150 pixels. Above this, I encountered issues with my machine crashing, and below this, the model accuracy was still lacking. X-Ray Images are grayscale by definition and defining them as such for the tensors increased the training time significantly. However, the final model I decided on used a base of an ‘inception’ model that had been trained on the Imagenet database, where the images are not grayscale but RGB, and in order to use those base layers for the network, the input images had to be RGB too.\r\nBatch size of about 32 seems to be pretty common for models trained on a CPU. Processing the image tensors in batches does not provide a signigicant advantage in machines with just a CPU as they cannot conduct batch processing. However, the keras library was defined to be compatible with GPUs, which do conduct batch processing, so we do need to define a batch size.\r\nThe number of iterations the network goes through the entire training dataset during the model training process is said to be the number of epochs of the model. Higher epochs lead to higher training times but more accurate models until overfitting takes over. 5 epochs provided the sweet spot of enough iterations to build an accurate enough model while avoiding overfitting and also without consuming a lot of time.\r\n\r\n\r\n# number of output classes \r\noutput_n <- 2\r\n\r\nimg_width <- 150\r\nimg_height <- 150\r\ntarget_size <- c(img_width, img_height)\r\n\r\n# RGB = 3 channels\r\n#grayscale = 1 channel\r\nchannels <- 3\r\n\r\nbatch_size <- 32\r\n\r\n# number of training epochs\r\nepochs <- 5\r\n\r\n\r\n\r\nThe images are then loaded into an object known as a data generator and their array values are divided by 255. This is because the pixel values are defined between 1 and 255 (8-bit) for each channel and we need these values to be between 0 and 1 for training our network.\r\nNext is a step known as data augmentation. This is where we take the images and make a variety of small changes to them to create a bunch of new images. This process increases the training data that we have, it reduces the chances of overfitting and also makes the model more robust in predicting images that may not be exactly similar in their format to the images that were used in training. This can be achieved by a variety of methods such as rotating the images slightly, stretching the images verticlly or horizontally, or stretching the image in some other direction(shearing), zooming, or flipping the images. It is important to the note that data augmentation is always only performed on the training data only.\r\nAs for the exact parameters for these methods, I replicated values which I came across in a textbook. You could perform some trial and error with them to see if it makes a significant change but this would take up a lot of time, and I decided that there were other changes to the model that promised more certain increases in performance.\r\n\r\n\r\ntrain_data_gen <- image_data_generator(\r\n  rescale            = 1/255    ,\r\n  # data augmentation\r\n  rotation_range     = 5        ,\r\n  width_shift_range  = 0.1      ,\r\n  height_shift_range = 0.05     ,\r\n  shear_range        = 0.1      ,\r\n  zoom_range         = 0.15     ,\r\n  horizontal_flip    = TRUE     ,\r\n  vertical_flip      = FALSE    ,\r\n  fill_mode          = \"reflect\"\r\n)\r\n\r\nvalidation_test_datagen <- image_data_generator(rescale = 1/255)\r\n\r\n\r\n\r\nThis is the where the image tensors are created and batched together to a form that is a compatible input to the neural network.\r\n\r\n\r\n# training images\r\ntrain_image_array_gen <- flow_images_from_directory(train_dir, \r\n                                          train_data_gen,\r\n                                          subset = 'training',\r\n                                          target_size = target_size,\r\n                                          class_mode = \"categorical\",\r\n                                          classes = c('NORMAL', 'PNEUMONIA'),\r\n                                          color_mode = 'rgb',\r\n                                          batch_size = batch_size,\r\n                                          seed = 42)\r\n\r\n\r\n\r\n\r\n\r\n# validation images\r\n\r\n\r\nvalid_image_array_gen <- flow_images_from_directory(\r\n  validation_dir                          ,\r\n  validation_test_datagen                 ,\r\n  classes     = c('NORMAL', 'PNEUMONIA')  ,\r\n  target_size = target_size ,\r\n  batch_size  = batch_size     ,\r\n  color_mode = 'rgb',\r\n  class_mode  = \"categorical\"             ,\r\n#  shuffle     = T                         ,\r\n  seed        = 42\r\n)\r\n\r\n\r\n\r\n\r\n\r\n# number of training samples\r\ntrain_samples <- train_image_array_gen$n\r\n\r\n# number of validation samples\r\nvalid_samples <- valid_image_array_gen$n\r\n\r\n\r\n\r\nThe model I settled with uses a concept called transfer learning. This is where you use a model that has previously been trained on some other non-related datset and use it as a convolutional base for your model. This base, while having no relationship to our dataset will have the capability to identify common features in images such as edges, loops, strokes and so on. The additional layers on top of this base would then specialize on features from images on our dataset to give us our complete model.\r\nInceptionV3 is one such pretrained model and I used the version of the model that was trained on a massive crowdsourced visual database that is commonly used in deep learning research known as ImageNet. This is a very simple implementation of this method.\r\nThe output of this base is a 3D tensor and we flatten that output into a 1D vector. We feed this 1D vector into 2 layers of densely connected neural network layers. Densely connected layers are those where all neurons in 1 layer are connected to all neurons in the other layers.\r\nThere is a lot of room in picking the number of units in a dense layer( 256 here) and it is basically a trial and error process to tune this number, which can be very time consuming. General advice was that for binary classification 1 layer of 256 units should be enough so I decided to adopt that. The final dense layer will be providing our classification output and so the number of units in this layer should match the number of classes in our classification task.\r\nThen we compile our model by defining the loss function, which is the function by which the network measures its performance while training. Binary cross-entropy is the go to function for binary classification tasks. It measures the distance of the classification probabilities from the 0 and 1 values, and then finds the average for this value over the entire training set.\r\nThe optimizer is the algorithm by which the network updates its weights according to the feedback from the loss function. The classic example of this is the Stochastic Gradient Descent method. While this method would work perfectly fine for this task, I opted to use the rmsprop function since it is considered to handle saddle points better when trying to find global minima in the loss function.\r\nThe metric is just the accuracy of the model in classifying the images. This is not used in training the model but to give a more intepretable measure of the model’s performance that the loss value.\r\n\r\n\r\n#inception model implementation\r\n\r\nconv_base <- application_inception_v3(\r\n  weights = \"imagenet\",\r\n  include_top = FALSE,\r\n  input_shape = c(img_width, img_height, channels)\r\n)\r\n\r\n\r\nmodel <- keras_model_sequential() %>%\r\n  conv_base %>%\r\n  layer_flatten() %>%\r\n  layer_dense(units = 256, activation = \"relu\") %>%\r\n  layer_dense(units = output_n, activation = \"sigmoid\") \r\n\r\n\r\nmodel %>% compile(\r\n  loss = \"binary_crossentropy\",\r\n  optimizer = optimizer_rmsprop(lr = 2e-5),\r\n  metrics = c(\"accuracy\")\r\n)\r\n\r\n\r\n\r\nThe final architecture of the model is displayed below.\r\n\r\n\r\nmodel <- load_model_hdf5(\"xray_model_inc.h5\")\r\nmodel\r\n\r\n\r\nModel\r\nModel: \"sequential_2\"\r\n______________________________________________________________________\r\nLayer (type)                   Output Shape                Param #    \r\n======================================================================\r\ninception_v3 (Functional)      (None, 3, 3, 2048)          21802784   \r\n______________________________________________________________________\r\nflatten_2 (Flatten)            (None, 18432)               0          \r\n______________________________________________________________________\r\ndense_5 (Dense)                (None, 256)                 4718848    \r\n______________________________________________________________________\r\ndense_4 (Dense)                (None, 2)                   514        \r\n======================================================================\r\nTotal params: 26,522,146\r\nTrainable params: 26,487,714\r\nNon-trainable params: 34,432\r\n______________________________________________________________________\r\n\r\nThen we start training the model. I’ve stopped this code from evaluating during knitting since it took several hours for the training to complete but I’ve loaded in the trained model above and will evaluate that model on the training dataset to display the training results.\r\n\r\n\r\nhist <- model %>% fit_generator(\r\n  # training data\r\n  train_image_array_gen,\r\n  \r\n  # epochs\r\n  steps_per_epoch = as.integer(train_samples / batch_size), \r\n  epochs = epochs, \r\n  \r\n  \r\n  # validation data\r\n  validation_data = valid_image_array_gen,\r\n  validation_steps = as.integer(valid_samples / batch_size)\r\n)\r\n\r\n\r\n\r\n\r\n\r\nmodel %>%\r\n  evaluate_generator(train_image_array_gen, \r\n                     steps = as.integer(train_samples / batch_size))\r\n\r\n\r\n      loss   accuracy \r\n0.07577854 0.98025304 \r\n\r\nWe’ve obtained a training accuracy of 0.98 which is pretty high for a basic model such as this, which leads us to believe we may be overfitting. We can check this by evaluating our model on the test images.\r\n\r\n\r\n#model %>% save_model_hdf5(\"xray_model_inc.h5\")\r\n#model <- load_model_hdf5(\"xray_model_inc.h5\")\r\n\r\n\r\n\r\nSetting up the test images.\r\n\r\n\r\ntest_datagen <- image_data_generator(rescale = 1/255)\r\n\r\ntest_generator <- flow_images_from_directory(\r\n        test_dir,\r\n        test_datagen,\r\n        target_size = target_size,\r\n        class_mode = \"categorical\",\r\n        classes = c('NORMAL', 'PNEUMONIA'),\r\n        color_mode = 'rgb',\r\n        batch_size = 1,\r\n        shuffle = FALSE,\r\n        seed = 42)\r\n\r\n\r\n\r\n\r\n\r\nmodel %>%\r\n  evaluate_generator(test_generator, \r\n                     steps = as.integer(test_generator$n))\r\n\r\n\r\n     loss  accuracy \r\n0.3985581 0.9102564 \r\n\r\nThe accuracy on our test images is 0.91, which confirms our suspicions of overfitting to the training data but is still pretty good and meets the 0.9 accuracy target we had set at the begining of the project.\r\n(The comments below are some notes I had made from previous modelling attempts.)\r\n\r\n\r\n# without aug,val=val 0.7152619 0.7884616 \r\n# after aug, val=val 0.4689246 0.7836539 \r\n# grayscale instead of rgb : loss about the same, accuracy up by 0.02\r\n# changed activation to sigmoid from soft max 0.4181049 0.8028846 (no change)\r\n# changed loss to binary cross_entropy        0.397662 0.823718 (up 0.02)\r\n#changed to 5 epochs 0.3537454 0.8381410\r\n#image size 100(instead of 20)   0.2994733 0.8814102\r\n#image size back to 20  0.3393419 0.8509616 \r\n#leaning rate 1e-5 0.4950307 0.8221154\r\n# learning rate 1e-5 10 epoch 0.3916168 0.8429487\r\n#learning rate 1e-4\r\n# inception model (trainloss: 0.1576 - accuracy: 0.9455) test: 0.4818154 0.8814102 (overfitting)\r\n\r\n# scr model 10 epoch, 150x150 : train: (loss: 0.1555 - accuracy: 0.9439) (test: 0.5519876 0.8381410 )\r\n# scr model 5 epoch, size 150x150 or 20x20: test: loss: 0.3339 - accuracy: 0.8542\r\n# scr model grayscale 5 epoch 100x100 loss: 0.2771 - accuracy: 0.8942\r\n\r\n#scr model rgb 5 epoch 100x100 : 0.85\r\n#scr model rgb 5 epoch 150x100: 0.84\r\n\r\n# inception model 5 epoch 150x150 loss: 0.3986 - accuracy: 0.9103 \r\n\r\n\r\n\r\nTo deconstruct these test image evaluations, we can build a confusion matrix. (This was a neat bit of code I found that managed this)\r\n\r\n\r\npreds   <- predict_generator(model,\r\n                             test_generator,\r\n                             steps = length(list.files(test_dir, recursive = T)))\r\n                            \r\npredictions                                                       <- data.frame(test_generator$filenames)\r\npredictions$prob_pneumonia                                        <- preds[,2]\r\ncolnames(predictions)                                             <- c('Filename', 'Prob_Pneumonia')\r\npredictions$Class_predicted                                       <- 'Normal'\r\npredictions$Class_predicted[predictions$Prob_Pneumonia >= 0.5]    <- 'Pneumonia'\r\npredictions$Class_actual                                          <- 'Normal'\r\npredictions$Class_actual[grep(\"PNEUMONIA\", predictions$Filename)] <- 'Pneumonia'\r\npredictions$Class_predicted                                       <- as.factor(predictions$Class_predicted )\r\npredictions$Class_actual                                          <- as.factor(predictions$Class_actual )\r\nconfusionMatrix(predictions$Class_predicted, predictions$Class_actual, positive = 'Pneumonia')\r\n\r\n\r\nConfusion Matrix and Statistics\r\n\r\n           Reference\r\nPrediction  Normal Pneumonia\r\n  Normal       185         7\r\n  Pneumonia     49       383\r\n                                         \r\n               Accuracy : 0.9103         \r\n                 95% CI : (0.885, 0.9315)\r\n    No Information Rate : 0.625          \r\n    P-Value [Acc > NIR] : < 2.2e-16      \r\n                                         \r\n                  Kappa : 0.8014         \r\n                                         \r\n Mcnemar's Test P-Value : 4.281e-08      \r\n                                         \r\n            Sensitivity : 0.9821         \r\n            Specificity : 0.7906         \r\n         Pos Pred Value : 0.8866         \r\n         Neg Pred Value : 0.9635         \r\n             Prevalence : 0.6250         \r\n         Detection Rate : 0.6138         \r\n   Detection Prevalence : 0.6923         \r\n      Balanced Accuracy : 0.8863         \r\n                                         \r\n       'Positive' Class : Pneumonia      \r\n                                         \r\n\r\nThe most dangerous case is where the model would classify an X-Ray image as normal, while the person actually does have pneumonia. The metric that reflects that is sensitivity, the true positive rate, which is 0.98 in this model which means that only about 2% of the cases where the person had pneumonia were incorrectly classified as ‘Normal’.\r\nMost of the errors were due to cases where the model would predict ‘Pneumonia’ on images that were actually ‘Normal’, which gave the comparatively lower Specificity of 0.79. Overall, this gave the model an accuracy of 0.91.\r\nThis model achieved the 2 objectives of an accuracy and sensitivity of over 0.9.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-08-17-xrayprojectpost/thumbnail_xray2.jpg",
    "last_modified": "2021-08-19T13:40:16-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-14-kju-trump/",
    "title": "Do you look more like Kim Jong Un or Donald Trump?",
    "description": "A Deep Learning Solution to the Popular Dilemna.",
    "author": [
      {
        "name": "Niketh Gamage",
        "url": {}
      }
    ],
    "date": "2021-04-14",
    "categories": [],
    "contents": "\r\nEver sit there relaxing after work and wondered “Do I look more like Kim Jong Un or Donald Trump?”. Now you can find the definitive answer.\r\nClick here and upload a picture of your face to find out. This might take about a minute to setup the model but I am in the process of deploying the application at a permanent home.\r\nThis model uses a Convolutional Neural Network (CNN) and was developed with the help of the fastaiv2 package.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-14-kju-trump/kju-trump-preview.jpg",
    "last_modified": "2021-04-14T14:42:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-06-lendingclubarticle/",
    "title": "Lending Club Model",
    "description": "A Loan Payback Prediction Model with lasso, random forest and knn.",
    "author": [
      {
        "name": "Niketh Gamage",
        "url": {}
      }
    ],
    "date": "2021-04-06",
    "categories": [],
    "contents": "\r\n\r\n\r\n# SEE modeldata package for new datasets\r\nlibrary(tidyverse)         # for graphing and data cleaning\r\nlibrary(tidymodels)        # for modeling\r\nlibrary(stacks)            # for stacking models\r\nlibrary(naniar)            # for examining missing values (NAs)\r\nlibrary(lubridate)         # for date manipulation\r\nlibrary(moderndive)        # for King County housing data\r\nlibrary(vip)               # for variable importance plots\r\nlibrary(DALEX)             # for model interpretation  \r\nlibrary(DALEXtra)          # for extension of DALEX\r\nlibrary(patchwork)         # for combining plots nicely\r\ntheme_set(theme_minimal()) # for theming\r\n\r\nlibrary(ranger) # for random forest model\r\nlibrary(kknn)   #for knn model\r\nlibrary(ROSE)  #for ROC curve\r\n\r\n\r\n\r\n\r\n\r\ndata(\"lending_club\")\r\n\r\n\r\n\r\nModeling\r\nWe will be using the lending_club dataset, which is available in the ‘modeldata’ library in the ‘tidymodels’ package, to predict the variable Class which shows if the loan is fully paid back/being paid back (category: good) or if it is defaulted/late(category: bad).\r\nWe will try to build 3 types of models - lasso, random forest and knn - and make a final optimized model that combines all of this information.\r\nSome data exploration and checking the distributions of the variables.\r\n\r\n\r\nlending_club \r\n\r\n\r\n# A tibble: 9,857 x 23\r\n   funded_amnt term    int_rate sub_grade addr_state verification_sta~\r\n         <int> <fct>      <dbl> <fct>     <fct>      <fct>            \r\n 1       16100 term_36    14.0  C4        CT         Not_Verified     \r\n 2       32000 term_60    12.0  C1        MN         Verified         \r\n 3       10000 term_36    16.3  D1        OH         Source_Verified  \r\n 4       16800 term_60    13.7  C3        NV         Verified         \r\n 5        3500 term_36     7.39 A4        CA         Source_Verified  \r\n 6       10000 term_36    11.5  B5        TX         Source_Verified  \r\n 7       11000 term_36     5.32 A1        KY         Not_Verified     \r\n 8       15000 term_36     9.16 B2        MO         Not_Verified     \r\n 9        6000 term_36     9.8  B3        NY         Source_Verified  \r\n10       20000 term_60    13.0  C2        GA         Not_Verified     \r\n# ... with 9,847 more rows, and 17 more variables: annual_inc <dbl>,\r\n#   emp_length <fct>, delinq_2yrs <int>, inq_last_6mths <int>,\r\n#   revol_util <dbl>, acc_now_delinq <int>, open_il_6m <int>,\r\n#   open_il_12m <int>, open_il_24m <int>, total_bal_il <int>,\r\n#   all_util <int>, inq_fi <int>, inq_last_12m <int>,\r\n#   delinq_amnt <int>, num_il_tl <int>,\r\n#   total_il_high_credit_limit <int>, Class <fct>\r\n\r\n\r\n\r\n# numeric variables\r\n\r\nlending_club %>% \r\n  select(where(is.numeric)) %>% \r\n  pivot_longer(cols = everything(),\r\n               names_to = \"variable\", \r\n               values_to = \"value\") %>% \r\n  ggplot(aes(x = value)) +\r\n  geom_histogram(bins = 30) +\r\n  facet_wrap(vars(variable), \r\n             scales = \"free\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n# categorical variables\r\n\r\nlending_club %>% \r\n  select(where(is.factor)) %>% \r\n  pivot_longer(cols = everything(),\r\n               names_to = \"variable\", \r\n               values_to = \"value\") %>% \r\n  ggplot(aes(x = value)) +\r\n  geom_bar() +\r\n  facet_wrap(vars(variable), \r\n             scales = \"free\", \r\n             nrow = 2)\r\n\r\n\r\n\r\n\r\nWe do some data cleaning steps to get rid of variables that are redundant/ identical to the chosen response variable ‘Class’.\r\n\r\n\r\n#get rid of zero or near zero variance variables\r\nlending_club2 <- lending_club %>% \r\n  na.omit() %>% \r\n  select(-delinq_amnt) %>% \r\n  select(-acc_now_delinq)\r\n\r\nlending_club\r\n\r\n\r\n# A tibble: 9,857 x 23\r\n   funded_amnt term    int_rate sub_grade addr_state verification_sta~\r\n         <int> <fct>      <dbl> <fct>     <fct>      <fct>            \r\n 1       16100 term_36    14.0  C4        CT         Not_Verified     \r\n 2       32000 term_60    12.0  C1        MN         Verified         \r\n 3       10000 term_36    16.3  D1        OH         Source_Verified  \r\n 4       16800 term_60    13.7  C3        NV         Verified         \r\n 5        3500 term_36     7.39 A4        CA         Source_Verified  \r\n 6       10000 term_36    11.5  B5        TX         Source_Verified  \r\n 7       11000 term_36     5.32 A1        KY         Not_Verified     \r\n 8       15000 term_36     9.16 B2        MO         Not_Verified     \r\n 9        6000 term_36     9.8  B3        NY         Source_Verified  \r\n10       20000 term_60    13.0  C2        GA         Not_Verified     \r\n# ... with 9,847 more rows, and 17 more variables: annual_inc <dbl>,\r\n#   emp_length <fct>, delinq_2yrs <int>, inq_last_6mths <int>,\r\n#   revol_util <dbl>, acc_now_delinq <int>, open_il_6m <int>,\r\n#   open_il_12m <int>, open_il_24m <int>, total_bal_il <int>,\r\n#   all_util <int>, inq_fi <int>, inq_last_12m <int>,\r\n#   delinq_amnt <int>, num_il_tl <int>,\r\n#   total_il_high_credit_limit <int>, Class <fct>\r\n\r\nlending_club2\r\n\r\n\r\n# A tibble: 9,857 x 21\r\n   funded_amnt term    int_rate sub_grade addr_state verification_sta~\r\n         <int> <fct>      <dbl> <fct>     <fct>      <fct>            \r\n 1       16100 term_36    14.0  C4        CT         Not_Verified     \r\n 2       32000 term_60    12.0  C1        MN         Verified         \r\n 3       10000 term_36    16.3  D1        OH         Source_Verified  \r\n 4       16800 term_60    13.7  C3        NV         Verified         \r\n 5        3500 term_36     7.39 A4        CA         Source_Verified  \r\n 6       10000 term_36    11.5  B5        TX         Source_Verified  \r\n 7       11000 term_36     5.32 A1        KY         Not_Verified     \r\n 8       15000 term_36     9.16 B2        MO         Not_Verified     \r\n 9        6000 term_36     9.8  B3        NY         Source_Verified  \r\n10       20000 term_60    13.0  C2        GA         Not_Verified     \r\n# ... with 9,847 more rows, and 15 more variables: annual_inc <dbl>,\r\n#   emp_length <fct>, delinq_2yrs <int>, inq_last_6mths <int>,\r\n#   revol_util <dbl>, open_il_6m <int>, open_il_12m <int>,\r\n#   open_il_24m <int>, total_bal_il <int>, all_util <int>,\r\n#   inq_fi <int>, inq_last_12m <int>, num_il_tl <int>,\r\n#   total_il_high_credit_limit <int>, Class <fct>\r\n\r\nWe saw earlier from the distribution of Class that there is a lot more good rows than bad. So, we resample with replacement some bad cases and add them to our dataset so that the good cases don’t overwhelm our classification model into predicting good all the time.\r\n\r\n\r\ncreate_more_bad <- lending_club2 %>% \r\n  filter(Class == \"bad\") %>% \r\n  sample_n(size = 3000, replace = TRUE)\r\n\r\nlending_club_mod <- lending_club2 %>% \r\n  bind_rows(create_more_bad)\r\n\r\n\r\n\r\nThen we split the data into training and testing set.\r\n\r\n\r\nset.seed(494) # for reproducibility\r\n\r\nlending_split <- initial_split(lending_club_mod, prop = 0.75)\r\n\r\nlending_training <- training(lending_split)\r\nlending_testing <- testing(lending_split)\r\n\r\n\r\n\r\nThen we create the recipe and pre-process the data to make it ready to build our first model: lasso.\r\n\r\n\r\nlending_recipe <- recipe(Class ~ . , data = lending_training) %>% \r\n  # making all integer variables are numeric\r\n  step_mutate_at(all_numeric(), fn = ~as.numeric(.)) %>% \r\n  \r\n   # making categorical variables dummy variables\r\n  step_dummy(all_nominal(),-all_outcomes()) %>% \r\n\r\n  #quantitative variables are normalized\r\n  step_normalize(all_predictors(), \r\n                 -all_nominal(),\r\n                 -has_role(match = 'evaluative'))  \r\n\r\n\r\n\r\nChecking if everything looks ok and normalized:\r\n\r\n\r\nlending_recipe %>% \r\n  prep(lending_training) %>% \r\n  juice()\r\n\r\n\r\n# A tibble: 9,643 x 113\r\n   funded_amnt int_rate annual_inc delinq_2yrs inq_last_6mths\r\n         <dbl>    <dbl>      <dbl>       <dbl>          <dbl>\r\n 1      0.0218   0.0789     -0.862      -0.367         -0.705\r\n 2      1.80    -0.301      -0.146      -0.367         -0.705\r\n 3     -0.659    0.516      -0.146      -0.367          1.44 \r\n 4      0.0999   0.0181      0.414      -0.367         -0.705\r\n 5     -1.38    -1.17       -0.570      -0.367         -0.705\r\n 6     -0.659   -0.400      -0.920      -0.367         -0.705\r\n 7     -0.547   -1.57       -0.282      -0.367         -0.705\r\n 8     -0.101   -0.838       2.10        1.82          -0.705\r\n 9     -1.11    -0.717       0.182      -0.367          1.44 \r\n10      0.457   -0.111      -0.611       0.729         -0.705\r\n# ... with 9,633 more rows, and 108 more variables: revol_util <dbl>,\r\n#   open_il_6m <dbl>, open_il_12m <dbl>, open_il_24m <dbl>,\r\n#   total_bal_il <dbl>, all_util <dbl>, inq_fi <dbl>,\r\n#   inq_last_12m <dbl>, num_il_tl <dbl>,\r\n#   total_il_high_credit_limit <dbl>, Class <fct>,\r\n#   term_term_60 <dbl>, sub_grade_A2 <dbl>, sub_grade_A3 <dbl>,\r\n#   sub_grade_A4 <dbl>, sub_grade_A5 <dbl>, sub_grade_B1 <dbl>,\r\n#   sub_grade_B2 <dbl>, sub_grade_B3 <dbl>, sub_grade_B4 <dbl>,\r\n#   sub_grade_B5 <dbl>, sub_grade_C1 <dbl>, sub_grade_C2 <dbl>,\r\n#   sub_grade_C3 <dbl>, sub_grade_C4 <dbl>, sub_grade_C5 <dbl>,\r\n#   sub_grade_D1 <dbl>, sub_grade_D2 <dbl>, sub_grade_D3 <dbl>,\r\n#   sub_grade_D4 <dbl>, sub_grade_D5 <dbl>, sub_grade_E1 <dbl>,\r\n#   sub_grade_E2 <dbl>, sub_grade_E3 <dbl>, sub_grade_E4 <dbl>,\r\n#   sub_grade_E5 <dbl>, sub_grade_F1 <dbl>, sub_grade_F2 <dbl>,\r\n#   sub_grade_F3 <dbl>, sub_grade_F4 <dbl>, sub_grade_F5 <dbl>,\r\n#   sub_grade_G1 <dbl>, sub_grade_G2 <dbl>, sub_grade_G3 <dbl>,\r\n#   sub_grade_G4 <dbl>, sub_grade_G5 <dbl>, addr_state_AL <dbl>,\r\n#   addr_state_AR <dbl>, addr_state_AZ <dbl>, addr_state_CA <dbl>,\r\n#   addr_state_CO <dbl>, addr_state_CT <dbl>, addr_state_DC <dbl>,\r\n#   addr_state_DE <dbl>, addr_state_FL <dbl>, addr_state_GA <dbl>,\r\n#   addr_state_HI <dbl>, addr_state_ID <dbl>, addr_state_IL <dbl>,\r\n#   addr_state_IN <dbl>, addr_state_KS <dbl>, addr_state_KY <dbl>,\r\n#   addr_state_LA <dbl>, addr_state_MA <dbl>, addr_state_MD <dbl>,\r\n#   addr_state_ME <dbl>, addr_state_MI <dbl>, addr_state_MN <dbl>,\r\n#   addr_state_MO <dbl>, addr_state_MS <dbl>, addr_state_MT <dbl>,\r\n#   addr_state_NC <dbl>, addr_state_ND <dbl>, addr_state_NE <dbl>,\r\n#   addr_state_NH <dbl>, addr_state_NJ <dbl>, addr_state_NM <dbl>,\r\n#   addr_state_NV <dbl>, addr_state_NY <dbl>, addr_state_OH <dbl>,\r\n#   addr_state_OK <dbl>, addr_state_OR <dbl>, addr_state_PA <dbl>,\r\n#   addr_state_RI <dbl>, addr_state_SC <dbl>, addr_state_SD <dbl>,\r\n#   addr_state_TN <dbl>, addr_state_TX <dbl>, addr_state_UT <dbl>,\r\n#   addr_state_VA <dbl>, addr_state_VT <dbl>, addr_state_WA <dbl>,\r\n#   addr_state_WI <dbl>, addr_state_WV <dbl>, addr_state_WY <dbl>,\r\n#   verification_status_Source_Verified <dbl>,\r\n#   verification_status_Verified <dbl>, emp_length_emp_1 <dbl>,\r\n#   emp_length_emp_ge_10 <dbl>, emp_length_emp_2 <dbl>, ...\r\n\r\nThen we set up the lasso model and workflow and set the penalty parameter to tune().\r\n\r\n\r\n#define lasso model\r\nlending_lasso_mod <- \r\n  logistic_reg(penalty = tune(), mixture = 1) %>% \r\n  set_engine(\"glmnet\") %>% \r\n  set_mode(\"classification\")\r\n\r\n\r\n\r\n\r\n\r\n# create workflow\r\nlending_lasso_wf <- \r\n  workflow() %>% \r\n  add_recipe(lending_recipe) %>% \r\n  add_model(lending_lasso_mod)\r\n\r\n\r\n\r\nSet up the model tuning for the penalty parameter. Be sure to add the control_stack_grid() for the control argument so we can use these results later when we stack. Find the accuracy and area under the roc curve for the model with the best tuning parameter. Use 5-fold cv.\r\nSetting up model tuning for the penalty parameter with 5-fold cross validation using the training dataset\r\n\r\n\r\nset.seed(494) #for reproducible 5-fold\r\n\r\nlending_cv <- vfold_cv(lending_training, v = 5)\r\n\r\npenalty_grid <- grid_regular(penalty(),\r\n                             levels = 10)\r\n\r\naccuracy_met <- metric_set(accuracy)\r\n\r\n# tune the model \r\nlending_lasso_tune <- \r\n  lending_lasso_wf %>% \r\n  tune_grid(\r\n    resamples = lending_cv,\r\n    grid = penalty_grid,\r\n    control = control_stack_grid()\r\n#    metrics = accuracy_met\r\n    )\r\n\r\n\r\n\r\nFinding the best tuning parameter and finalizing the model.\r\n\r\n\r\n# Best tuning parameter by smallest rmse\r\nbest_param <- lending_lasso_tune %>% \r\n  select_best(metric = \"accuracy\")\r\n\r\nlending_lasso_final_wf <- lending_lasso_wf %>% \r\n  finalize_workflow(best_param)\r\n\r\nlending_lasso_final_mod <- lending_lasso_final_wf %>% \r\n  fit(data = lending_training)\r\n\r\n\r\n\r\nLet’s take a look at the model estimates for our predictors\r\n\r\n\r\nlending_lasso_final_mod %>% \r\n  pull_workflow_fit() %>% \r\n  tidy() \r\n\r\n\r\n# A tibble: 113 x 3\r\n   term           estimate penalty\r\n   <chr>             <dbl>   <dbl>\r\n 1 (Intercept)      1.16   0.00599\r\n 2 funded_amnt      0      0.00599\r\n 3 int_rate        -0.776  0.00599\r\n 4 annual_inc      -0.0471 0.00599\r\n 5 delinq_2yrs      0      0.00599\r\n 6 inq_last_6mths  -0.0716 0.00599\r\n 7 revol_util       0      0.00599\r\n 8 open_il_6m       0      0.00599\r\n 9 open_il_12m     -0.185  0.00599\r\n10 open_il_24m      0      0.00599\r\n# ... with 103 more rows\r\n\r\nThen we fit the model with the testing data and check the metrics.\r\n\r\n\r\n# Fit model with best tuning parameter(s) to training data and apply to test data\r\nlending_lasso_test <- lending_lasso_final_wf %>% \r\n  last_fit(lending_split)\r\n\r\n# Metrics for model applied to test data\r\nlending_lasso_test %>% \r\n  collect_metrics()\r\n\r\n\r\n# A tibble: 2 x 4\r\n  .metric  .estimator .estimate .config             \r\n  <chr>    <chr>          <dbl> <chr>               \r\n1 accuracy binary         0.762 Preprocessor1_Model1\r\n2 roc_auc  binary         0.779 Preprocessor1_Model1\r\n\r\nNot amazing but not too bad either. Now let’s move on to the random forest model.\r\nSetting up the recipe and the pre-processing steps to build a random forest model.\r\n\r\n\r\n# set up recipe and transformation steps and roles\r\nlendingrf_recipe <- \r\n  recipe(formula = Class ~ ., \r\n         data = lending_training) %>% \r\n  step_mutate_at(all_numeric(), \r\n            fn= ~as.numeric(.)) \r\n\r\n\r\n\r\nSetting up the random forest model and workflow and we tune over min_n and mtry\r\n\r\n\r\n#define model\r\nlendingrf_spec <- \r\n  rand_forest(mtry = tune(), \r\n              min_n = tune(), \r\n              trees = 100) %>% \r\n  set_mode(\"classification\") %>% \r\n  set_engine(\"ranger\")\r\n\r\n#create workflow\r\nlendingrf_wf <- \r\n  workflow() %>% \r\n  add_recipe(lendingrf_recipe) %>% \r\n  add_model(lendingrf_spec) \r\n\r\n\r\n\r\nTuning the model using 5 fold cross-validation\r\n\r\n\r\n#fit the model\r\nset.seed(494) # for reproducibility - random sampling in random forest choosing number of variables\r\n\r\n\r\nrfpenalty_grid <- grid_regular(finalize(mtry(), lending_training %>% select(-Class)), min_n(), levels = 3)\r\n\r\n\r\n\r\nlendingrf_tune <-\r\n  lendingrf_wf %>% \r\n  tune_grid(\r\n    resamples = lending_cv,\r\n    control = control_stack_grid(),\r\n    grid = rfpenalty_grid)\r\n\r\n\r\n\r\nFinalizing the model and fitting it to the testing data\r\n\r\n\r\n# Best tuning parameter by smallest rmse\r\nbestrf_param <- lendingrf_tune %>% \r\n  select_best(metric = \"accuracy\")\r\n\r\nlendingrf_final_wf <- lendingrf_wf %>% \r\n  finalize_workflow(bestrf_param)\r\n\r\n\r\nlendingrf_final_mod<- lendingrf_final_wf %>% \r\n  fit(lending_training)\r\n  \r\nlendingrf_last_fit <- lendingrf_final_wf %>% \r\n  last_fit(lending_split) \r\n\r\n\r\n\r\nMetrics for the finalized random forest model\r\n\r\n\r\nlendingrf_last_fit%>% \r\n  collect_metrics()\r\n\r\n\r\n# A tibble: 2 x 4\r\n  .metric  .estimator .estimate .config             \r\n  <chr>    <chr>          <dbl> <chr>               \r\n1 accuracy binary         0.994 Preprocessor1_Model1\r\n2 roc_auc  binary         0.998 Preprocessor1_Model1\r\n\r\nThen we use the DALEX and DALEXtra libraries to build plots of the residuals of each of the models.\r\n\r\n\r\nlasso_explain <-\r\n  explain_tidymodels(\r\n    model = lending_lasso_final_mod,\r\n    data = lending_training %>% select(-Class),\r\n    y = as.numeric(lending_training %>%  pull(Class)),\r\n    label = \"lasso\"\r\n  )\r\n\r\n\r\nPreparation of a new explainer is initiated\r\n  -> model label       :  lasso \r\n  -> data              :  9643  rows  20  cols \r\n  -> data              :  tibble converted into a data.frame \r\n  -> target variable   :  9643  values \r\n  -> predict function  :  yhat.workflow  will be used ( [33m default [39m )\r\n  -> predicted values  :  No value for predict function target column. ( [33m default [39m )\r\n  -> model_info        :  package tidymodels , ver. 0.1.2 , task classification ( [33m default [39m ) \r\n  -> predicted values  :  numerical, min =  0.02582564 , mean =  0.7258135 , max =  0.9798892  \r\n  -> residual function :  difference between y and yhat ( [33m default [39m )\r\n  -> residuals         :  numerical, min =  0.05164211 , mean =  0.9999979 , max =  1.974174  \r\n [32m A new explainer has been created! [39m \r\n\r\nrf_explain <- \r\n  explain_tidymodels(\r\n    model = lendingrf_final_mod,\r\n    data = lending_training %>% select(-Class), \r\n    y = as.numeric(lending_training %>%  pull(Class)),\r\n    label = \"rf\"\r\n  )\r\n\r\n\r\nPreparation of a new explainer is initiated\r\n  -> model label       :  rf \r\n  -> data              :  9643  rows  20  cols \r\n  -> data              :  tibble converted into a data.frame \r\n  -> target variable   :  9643  values \r\n  -> predict function  :  yhat.workflow  will be used ( [33m default [39m )\r\n  -> predicted values  :  No value for predict function target column. ( [33m default [39m )\r\n  -> model_info        :  package tidymodels , ver. 0.1.2 , task classification ( [33m default [39m ) \r\n  -> predicted values  :  numerical, min =  0 , mean =  0.702578 , max =  1  \r\n  -> residual function :  difference between y and yhat ( [33m default [39m )\r\n  -> residuals         :  numerical, min =  0.54 , mean =  1.023233 , max =  1.32  \r\n [32m A new explainer has been created! [39m \r\n\r\n\r\n\r\nlasso_mod_perf <- model_performance(lasso_explain)\r\nrf_mod_perf <-  model_performance(rf_explain)\r\n\r\n\r\n\r\n\r\n\r\nhist_plot <- \r\n  plot(lasso_mod_perf,\r\n       rf_mod_perf, \r\n       geom = \"histogram\")\r\nbox_plot <-\r\n  plot(lasso_mod_perf,\r\n       rf_mod_perf, \r\n       geom = \"boxplot\")\r\n\r\nhist_plot + box_plot\r\n\r\n\r\n\r\n\r\nCreating a variable importance plot to check out the most significant predictors.\r\n\r\n\r\nset.seed(494) #since we are sampling & permuting, we set a seed so we can replicate the results\r\nlasso_var_imp <- \r\n  model_parts(\r\n    lasso_explain\r\n    )\r\n\r\nplot(lasso_var_imp, show_boxplots = TRUE)\r\n\r\n\r\n\r\nset.seed(10) #since we are sampling & permuting, we set a seed so we can replicate the results\r\nrf_var_imp <- \r\n  model_parts(\r\n    rf_explain\r\n    )\r\n\r\nplot(rf_var_imp, show_boxplots = TRUE)\r\n\r\n\r\n\r\n\r\nInterest rate, annual income and verification status seem to be very important predictors which is not all that suprising.\r\nNow we set up the knn model and tune k.\r\n\r\n\r\n# create a model definition\r\nknn_mod <-\r\n  nearest_neighbor(\r\n    neighbors = tune(\"k\")\r\n  ) %>%\r\n  set_engine(\"kknn\") %>% \r\n  set_mode(\"classification\")\r\n\r\n# create the workflow\r\nknn_wf <- \r\n  workflow() %>% \r\n  add_model(knn_mod) %>%\r\n  add_recipe(lending_recipe)\r\n\r\n# tune it using 4 tuning parameters\r\nknn_tune <- \r\n  knn_wf %>% \r\n  tune_grid(\r\n    lending_cv,\r\n    grid = 4,\r\n    control = control_stack_grid()\r\n  )\r\n\r\n\r\n\r\nNow we stack the 3 model tuning parameters and ‘blend’ them to optimize our final model.\r\n\r\n\r\nlending_stack <- \r\n  stacks() %>% \r\n   add_candidates(lendingrf_tune) %>% \r\n   add_candidates(lending_lasso_tune) %>% \r\n   add_candidates(knn_tune)\r\n\r\nas_tibble(lending_stack)\r\n\r\n\r\n# A tibble: 9,643 x 37\r\n   Class .pred_bad_lendingrf~ .pred_bad_lendingrf~ .pred_bad_lendingr~\r\n   <fct>                <dbl>                <dbl>               <dbl>\r\n 1 good                0.180                0.176               0.163 \r\n 2 good                0.156                0.194               0.203 \r\n 3 good                0.197                0.314               0.345 \r\n 4 good                0.0905               0.178               0.170 \r\n 5 good                0.114                0.102               0.121 \r\n 6 good                0.115                0.151               0.126 \r\n 7 good                0.0555               0.0656              0.0641\r\n 8 good                0.0942               0.0796              0.150 \r\n 9 good                0.0949               0.177               0.115 \r\n10 good                0.0665               0.0781              0.0746\r\n# ... with 9,633 more rows, and 33 more variables:\r\n#   .pred_bad_lendingrf_tune_1_2 <dbl>,\r\n#   .pred_bad_lendingrf_tune_1_5 <dbl>,\r\n#   .pred_bad_lendingrf_tune_1_8 <dbl>,\r\n#   .pred_bad_lendingrf_tune_1_3 <dbl>,\r\n#   .pred_bad_lendingrf_tune_1_6 <dbl>,\r\n#   .pred_bad_lendingrf_tune_1_9 <dbl>,\r\n#   .pred_good_lendingrf_tune_1_1 <dbl>,\r\n#   .pred_good_lendingrf_tune_1_4 <dbl>,\r\n#   .pred_good_lendingrf_tune_1_7 <dbl>,\r\n#   .pred_good_lendingrf_tune_1_2 <dbl>,\r\n#   .pred_good_lendingrf_tune_1_5 <dbl>,\r\n#   .pred_good_lendingrf_tune_1_8 <dbl>,\r\n#   .pred_good_lendingrf_tune_1_3 <dbl>,\r\n#   .pred_good_lendingrf_tune_1_6 <dbl>,\r\n#   .pred_good_lendingrf_tune_1_9 <dbl>,\r\n#   .pred_bad_lending_lasso_tune_1_01 <dbl>,\r\n#   .pred_bad_lending_lasso_tune_1_07 <dbl>,\r\n#   .pred_bad_lending_lasso_tune_1_08 <dbl>,\r\n#   .pred_bad_lending_lasso_tune_1_09 <dbl>,\r\n#   .pred_bad_lending_lasso_tune_1_10 <dbl>,\r\n#   .pred_good_lending_lasso_tune_1_01 <dbl>,\r\n#   .pred_good_lending_lasso_tune_1_07 <dbl>,\r\n#   .pred_good_lending_lasso_tune_1_08 <dbl>,\r\n#   .pred_good_lending_lasso_tune_1_09 <dbl>,\r\n#   .pred_good_lending_lasso_tune_1_10 <dbl>,\r\n#   .pred_bad_knn_tune_1_1 <dbl>, .pred_bad_knn_tune_1_2 <dbl>,\r\n#   .pred_bad_knn_tune_1_3 <dbl>, .pred_bad_knn_tune_1_4 <dbl>,\r\n#   .pred_good_knn_tune_1_1 <dbl>, .pred_good_knn_tune_1_2 <dbl>,\r\n#   .pred_good_knn_tune_1_3 <dbl>, .pred_good_knn_tune_1_4 <dbl>\r\n\r\n\r\n\r\nlending_blend <- \r\n  lending_stack %>% \r\n  blend_predictions() \r\n\r\n\r\n\r\n\r\n\r\nautoplot(lending_blend)\r\n\r\n\r\n\r\n\r\n\r\n\r\nlending_blend\r\n\r\n\r\n# A tibble: 3 x 3\r\n  member                        type             weight\r\n  <chr>                         <chr>             <dbl>\r\n1 .pred_good_lendingrf_tune_1_2 rand_forest      4.87  \r\n2 .pred_good_knn_tune_1_1       nearest_neighbor 0.0335\r\n3 .pred_good_lendingrf_tune_1_3 rand_forest      0.0202\r\n\r\nWe then create the model stack and apply it onto our test data to see how it performs.\r\n\r\n\r\nlending_final_stack <- lending_blend %>% \r\n  fit_members()\r\n\r\n\r\n\r\n\r\n\r\n  lending_final_stack %>% \r\n  predict(new_data = lending_testing) %>% \r\n  bind_cols(lending_testing)\r\n\r\n\r\n# A tibble: 3,214 x 22\r\n   .pred_class funded_amnt term    int_rate sub_grade addr_state\r\n   <fct>             <int> <fct>      <dbl> <fct>     <fct>     \r\n 1 good              25750 term_36    13.7  C3        MI        \r\n 2 good              15000 term_60    15.8  D1        TX        \r\n 3 good              20000 term_36    10.8  B4        TN        \r\n 4 good              19350 term_36    11.5  B5        MI        \r\n 5 good              15000 term_36    10.8  B4        TX        \r\n 6 good              20400 term_60    14.5  C4        GA        \r\n 7 good              22000 term_60    13.7  C3        MN        \r\n 8 good              27200 term_60    10.8  B4        NC        \r\n 9 good              10000 term_36     6.49 A2        NY        \r\n10 good              30000 term_60    20.8  E2        IL        \r\n# ... with 3,204 more rows, and 16 more variables:\r\n#   verification_status <fct>, annual_inc <dbl>, emp_length <fct>,\r\n#   delinq_2yrs <int>, inq_last_6mths <int>, revol_util <dbl>,\r\n#   open_il_6m <int>, open_il_12m <int>, open_il_24m <int>,\r\n#   total_bal_il <int>, all_util <int>, inq_fi <int>,\r\n#   inq_last_12m <int>, num_il_tl <int>,\r\n#   total_il_high_credit_limit <int>, Class <fct>\r\n\r\n\r\n\r\npred_compare <- lending_final_stack %>% \r\n  predict(new_data = lending_testing) %>% \r\n  bind_cols(lending_testing) %>% \r\n  select(.pred_class,Class) \r\n\r\n\r\n\r\n\r\n\r\npredictions <- pred_compare %>% \r\n  pull(.pred_class)\r\ntrue_class <- pred_compare %>% \r\n  pull(Class)\r\n\r\n\r\n\r\n\r\n\r\nroc.curve(true_class, predictions)\r\n\r\n\r\n\r\nArea under the curve (AUC): 0.992\r\n\r\nWe get an auc_roc of 0.95 which is very very good for a classification model of this nature.\r\nShiny app\r\nWork In Progress.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-06-lendingclubarticle/Lending_Model_preview.png",
    "last_modified": "2021-04-14T22:56:12-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-06-tidytextarticle/",
    "title": "TidyText Tutorial",
    "description": "An introductory tutorial with excercises about the TidyText package",
    "author": [
      {
        "name": "Colleen Minnihan, Jackson Tak, Niketh Gamage",
        "url": {}
      }
    ],
    "date": "2021-04-06",
    "categories": [],
    "contents": "\r\n1. Tutorial\r\nIntroduction\r\nThe tidytext packages allows us to effectively do text mining tasks by converting text data into tidy formats. We can then incorporated these tidy data sets with other tools in data science or machine learning.\r\nIn this tutorial, we will explain some of the key functions used in the tidytext package by using a simple example. We will then ask you to work on questions using a different data set: Russian Troll Tweets.\r\n\r\nGetting Started\r\nLoading Libraries\r\nFirst, as always, we have to load libraries and install packages. You will encounter some familiar packages, but the main libraries required to do text mining with tidytext are listed below:\r\n\r\n\r\n\r\n\r\nLooking at the Data Set\r\nAfter loading all the relevant libraries, we will look at a simple data set that we created. The text data set includes 9 different sentences or strings.\r\n\r\n[1] \"Lisa is awesome\"         \"I love Data Science\"    \r\n[3] \"I hate cilantro\"         \"I dislike vegetables\"   \r\n[5] \"I enjoy smiling\"         \"I hate exams\"           \r\n[7] \"I love travelling\"       \"The weather is so nice!\"\r\n[9] \"Can I have an apple?\"   \r\n\r\nThe first task before doing text analysis is converting our data set into a tibble data frame by using the tibble function. A tibble is just another class of data frames in R, which allows us to work with tidy functions, as it does not convert strings to factors or use row names.\r\nTake a look at how we were able to achieve this below:\r\n\r\n# A tibble: 9 x 2\r\n   line text                   \r\n  <int> <chr>                  \r\n1     1 Lisa is awesome        \r\n2     2 I love Data Science    \r\n3     3 I hate cilantro        \r\n4     4 I dislike vegetables   \r\n5     5 I enjoy smiling        \r\n6     6 I hate exams           \r\n7     7 I love travelling      \r\n8     8 The weather is so nice!\r\n9     9 Can I have an apple?   \r\n\r\nAfter we successfully convert our data set into a tibble data frame, we want to extract individual words and put them into a data frame so that each row has one token.\r\nEach token can be interpreted as a word or unit of text. By utilizing tidytext’s unnest_tokens function, we can break the text into individual tokens. This process is also known as the tokenization process. Note that this function eliminates all the punctuation marks.\r\n\r\n# A tibble: 32 x 2\r\n    line word    \r\n   <int> <chr>   \r\n 1     1 lisa    \r\n 2     1 is      \r\n 3     1 awesome \r\n 4     2 i       \r\n 5     2 love    \r\n 6     2 data    \r\n 7     2 science \r\n 8     3 i       \r\n 9     3 hate    \r\n10     3 cilantro\r\n# ... with 22 more rows\r\n\r\nAs we see above, the unnest_tokens function takes in two parameters. The first parameter, the output column, will be the name of the output column or individual words. In our exercise, we will call it word.\r\nThe second parameter will take the tokenized column name from the tibble data frame.\r\nBefore starting our text analysis, we will also have to exclude stop words such as “is”, “I”, “the”, “about”, “an”, “the”, etc. The tidytext package provides a tibble data set called stop_words that includes different stop words.\r\nYou can take a peek at some of the stop words in the stop_words data set:\r\n\r\n# A tibble: 6 x 2\r\n  word      lexicon\r\n  <chr>     <chr>  \r\n1 a         SMART  \r\n2 a's       SMART  \r\n3 able      SMART  \r\n4 about     SMART  \r\n5 above     SMART  \r\n6 according SMART  \r\n\r\nFinally, we will use a familiar function, anti_join, to clean the data set. This is the last step of the data cleaning process that we need to do before moving forward with the analysis. By joining the two data sets, stop_words and text_unnest, we can arrive at a clean data set.\r\n\r\n# A tibble: 18 x 2\r\n    line word      \r\n   <int> <chr>     \r\n 1     1 lisa      \r\n 2     1 awesome   \r\n 3     2 love      \r\n 4     2 data      \r\n 5     2 science   \r\n 6     3 hate      \r\n 7     3 cilantro  \r\n 8     4 dislike   \r\n 9     4 vegetables\r\n10     5 enjoy     \r\n11     5 smiling   \r\n12     6 hate      \r\n13     6 exams     \r\n14     7 love      \r\n15     7 travelling\r\n16     8 weather   \r\n17     8 nice      \r\n18     9 apple     \r\n\r\n\r\nData Analysis\r\nWith the cleaned version of the data, we will begin our analysis. In this tutorial, we mainly focus on the sentiment analysis and creating relevant visualizations using word cloud\r\n\r\nSentiment Analysis\r\nBefore beginning the sentiment analysis, let’s take a step back and understand the purpose of sentiment analysis. Sentiment analysis “provides a way to understand the attitudes and opinions expressed in texts”.\r\nTo conduct sentiment analysis, we will be first looking at the get_sentiments(). By using this function within the tidytext package, we can look at words with negative and positive sentiments. Note that this function takes in a lexicon parameter. There are three sentiment lexicons that we can use:\r\nbing (default): positive vs. negative\r\nnrc: assigns yes vs. no to positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust cateogries.\r\nAFINN: assigns scores from -5 to 5 depending on the sentiment of the word.\r\nFor this tutorial, we will use bing for simplicity. Feel free to play with other lexicons.\r\n\r\n# A tibble: 6 x 2\r\n  word       sentiment\r\n  <chr>      <chr>    \r\n1 2-faces    negative \r\n2 abnormal   negative \r\n3 abolish    negative \r\n4 abominable negative \r\n5 abominably negative \r\n6 abominate  negative \r\n# A tibble: 6 x 2\r\n  word       sentiment\r\n  <chr>      <chr>    \r\n1 abound     positive \r\n2 abounds    positive \r\n3 abundance  positive \r\n4 abundant   positive \r\n5 accessable positive \r\n6 accessible positive \r\n\r\nNext, to figure out the sentiment for each word, we will join the two data sets: sentiments and text_clean. Note that some words in the text_clean data set are dropped i.e., “exams”, “weather”, “apple”, etc.\r\n\r\n# A tibble: 9 x 3\r\n   line word    sentiment\r\n  <int> <chr>   <chr>    \r\n1     1 awesome positive \r\n2     2 love    positive \r\n3     3 hate    negative \r\n4     4 dislike negative \r\n5     5 enjoy   positive \r\n6     5 smiling positive \r\n7     6 hate    negative \r\n8     7 love    positive \r\n9     8 nice    positive \r\n\r\nUsing this sentiment data, we will now begin the sentiment analysis by using some functions that we already know.\r\n\r\n# A tibble: 2 x 2\r\n  sentiment     n\r\n* <chr>     <int>\r\n1 negative      3\r\n2 positive      6\r\n\r\nFrom above, We see that there are 3 negative words and 6 positive words.\r\n\r\nMaking cool visualizations using WordCloud\r\nWith the above analysis in mind, we can also create interesting visualizations using the wordcloud package.\r\nUsing the text_clean data (without sentiments), we can create this nice visualization which shows us all the words in the data set. Note that the size of “hate” and “love” are bigger than other words. This is because they appear more than other words.\r\n\r\n\r\n\r\nUsing the data set that includes sentiments, we can also create a similar visualization.\r\n\r\n\r\n\r\nCompared to the first wordcloud visualization, this one allows us to make a comparison. Through this visualization, we can understand what words have positive and negative associations and what words appear more than the others within their specific groups.\r\n\r\nOther notable functions in the tidytext package\r\nFor the purpose of this tutorial, we selected some of the tidytext functions. However, there are many more functions that you might be interested in using for your final project if your group were to do some text mining and analysis.\r\nOther functions and their documentations can be found in the link below:\r\nhttps://cran.r-project.org/web/packages/tidytext/tidytext.pdf\r\n\r\n2. Exercises\r\nNow you will try using tidytext on a new dataset about Russian Troll tweets.\r\n\r\nRead about the data\r\nThese are tweets from Twitter handles that are connected to the Internet Research Agency (IRA), a Russian “troll factory.” The majority of these tweets were posted from 2015-2017, but the datasets encompass tweets from February 2012 to May 2018.\r\nThree of the main categories of troll tweet that we will be focusing on are Left Trolls, Right Trolls, and News Feed. Left Trolls usually pretend to be BLM activists, aiming to divide the democratic party (in this context, being pro-Bernie so that votes are taken away from Hillary). Right trolls imitate Trump supporters, and News Feed handles are “local news aggregators,” typically linking to legitimate news.\r\nFor our upcoming analyses, some important variables are:\r\nauthor (handle sending the tweet)\r\ncontent (text of the tweet)\r\nlanguage (language of the tweet)\r\npublish_date (date and time the tweet was sent)\r\nVariable documentation can be found on Github and a more detailed description of the dataset can be found in this fivethirtyeight article.\r\nBecause there are 12 datasets containing 2,973,371 tweets sent by 2,848 Twitter handles in total, we will be using three of these datasets (one from a Right troll, one from a Left troll, and one from a News Feed account).\r\n\r\n1. Read in Troll Tweets Dataset\r\n\r\n\r\n\r\n\r\n2. Basic Data Cleaning and Exploration\r\nRemove rows where the tweet was in a language other than English\r\nReport the dimensions of the dataset\r\nCreate two or three basic exploratory plots of the data (ex. plot of the different locations from which tweets were posted, plot of the account category of a tweet)\r\n\r\n\r\n\r\n\r\n3. Unnest Tokens\r\nWe want each row to represent a word from a tweet, rather than an entire tweet.\r\n\r\n\r\n\r\n\r\n4. Remove stopwords\r\n\r\n\r\n\r\nTake a look at the troll_tweets_cleaned dataset. Are there any other words/letters/numbers that we want to eliminate that weren’t taken care of by stop_words?\r\n\r\n\r\n\r\n\r\n5. Look at a subset of the tweets to see how often the top words appear.\r\n\r\n\r\n\r\n\r\n6. Sentiment Analysis\r\nGet the sentiments using the “bing” parameter (which classifies words into “positive” or “negative”)\r\nReport how many positive and negative words there are in the dataset. Are there more positive or negative words, and why do you think this might be?\r\n\r\n\r\n\r\n\r\n\r\n\r\n7. Using the troll_tweets_small dataset, make a wordcloud:\r\nThat is sized by the number of times that a word appears in the tweets\r\nThat is colored by sentiment (positive or negative)\r\n\r\n\r\n\r\nAre there any words whose categorization as “positive” or “negative” surprised you?\r\n\r\nSources\r\nhttps://cran.r-project.org/web/packages/tidytext/tidytext.pdf\r\nhttps://www.tidytextmining.com/\r\nhttps://fivethirtyeight.com/features/why-were-sharing-3-million-russian-troll-tweets\r\nhttps://www.youtube.com/watch?v=-uVo0Xvmimw\r\nhttps://github.com/fivethirtyeight/russian-troll-tweets/\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-06-tidytextarticle/tidytextarticle_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2021-04-16T02:50:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-04-nuclearexplosions/",
    "title": "Nuclear Explosions",
    "description": "This a small story about nuclear detonations told with some exploratory data visualization and analysis.",
    "author": [
      {
        "name": "Niketh Gamage",
        "url": {}
      }
    ],
    "date": "2021-04-04",
    "categories": [],
    "contents": "\r\nOverview\r\nNuclear weapons are the deadliest weapons known to man. In this dataset(which can be found here), we will be exploring the history of nuclear weapons detonation in the world. The data used for this analysis spans from 1945 - 1998, so the results of the analysis presented here are as of 1998.\r\nI will go about answering these questions:\r\nWhat are the countries with nuclear weapons capability and when did they achieve this?\r\nWhich country has detonated the most nuclear weapons?\r\nHow have these detonation patterns varied over time - what effect did the fall of the Soviet Union have on this?\r\nWhere were these nuclear weapons detonated?\r\nWhich countries definitevely have a fusion bomb(2nd generation of nukes)?\r\nHow did methods of testing them change over time?\r\nSimple application that lets you compare the countries nuclear detonation activity over time.\r\n1. When did each country first test a nuclear weapon?\r\n\r\n\r\n{\"x\":{\"data\":[{\"x\":[1942.35,2000.65],\"y\":[0,0],\"text\":\"yintercept: 0\",\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.13385826771654,\"color\":\"rgba(0,0,0,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1964],\"y\":[0],\"text\":\"first_bomb: 1964<br />y: 0<br />country: CHINA\",\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(248,118,109,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(248,118,109,1)\"}},\"hoveron\":\"points\",\"name\":\"CHINA\",\"legendgroup\":\"CHINA\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1960],\"y\":[0],\"text\":\"first_bomb: 1960<br />y: 0<br />country: FRANCE\",\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(196,154,0,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(196,154,0,1)\"}},\"hoveron\":\"points\",\"name\":\"FRANCE\",\"legendgroup\":\"FRANCE\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1974],\"y\":[0],\"text\":\"first_bomb: 1974<br />y: 0<br />country: INDIA\",\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(83,180,0,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(83,180,0,1)\"}},\"hoveron\":\"points\",\"name\":\"INDIA\",\"legendgroup\":\"INDIA\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1998],\"y\":[0],\"text\":\"first_bomb: 1998<br />y: 0<br />country: PAKIST\",\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,192,148,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,192,148,1)\"}},\"hoveron\":\"points\",\"name\":\"PAKIST\",\"legendgroup\":\"PAKIST\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1952],\"y\":[0],\"text\":\"first_bomb: 1952<br />y: 0<br />country: UK\",\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,182,235,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,182,235,1)\"}},\"hoveron\":\"points\",\"name\":\"UK\",\"legendgroup\":\"UK\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1945],\"y\":[0],\"text\":\"first_bomb: 1945<br />y: 0<br />country: USA\",\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(165,138,255,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(165,138,255,1)\"}},\"hoveron\":\"points\",\"name\":\"USA\",\"legendgroup\":\"USA\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1949],\"y\":[0],\"text\":\"first_bomb: 1949<br />y: 0<br />country: USSR\",\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(251,97,215,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(251,97,215,1)\"}},\"hoveron\":\"points\",\"name\":\"USSR\",\"legendgroup\":\"USSR\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":45.4063926940639,\"r\":7.30593607305936,\"b\":27.2146118721461,\"l\":10.958904109589},\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"title\":{\"text\":\"First Nuclear Detonation by Country\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":17.5342465753425},\"x\":0,\"xref\":\"paper\"},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[1942.35,2000.65],\"tickmode\":\"array\",\"ticktext\":[\"1950\",\"1960\",\"1970\",\"1980\",\"1990\",\"2000\"],\"tickvals\":[1950,1960,1970,1980,1990,2000],\"categoryorder\":\"array\",\"categoryarray\":[\"1950\",\"1960\",\"1970\",\"1980\",\"1990\",\"2000\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":3.65296803652968,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-0.05,0.05],\"tickmode\":\"array\",\"ticktext\":[\"-0.050\",\"-0.025\",\"0.000\",\"0.025\",\"0.050\"],\"tickvals\":[-0.05,-0.025,0,0.025,0.05],\"categoryorder\":\"array\",\"categoryarray\":[\"-0.050\",\"-0.025\",\"0.000\",\"0.025\",\"0.050\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":3.65296803652968,\"tickwidth\":0,\"showticklabels\":false,\"tickfont\":{\"color\":null,\"family\":null,\"size\":0},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":true,\"legend\":{\"bgcolor\":null,\"bordercolor\":null,\"borderwidth\":0,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895},\"y\":0.959399606299213},\"annotations\":[{\"text\":\"country\",\"x\":1.02,\"y\":1,\"showarrow\":false,\"ax\":0,\"ay\":0,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xref\":\"paper\",\"yref\":\"paper\",\"textangle\":-0,\"xanchor\":\"left\",\"yanchor\":\"bottom\",\"legendTitle\":true}],\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"141842d3a9a\":{\"yintercept\":{},\"type\":\"scatter\"},\"141842046b43\":{\"x\":{},\"y\":{},\"colour\":{}}},\"cur_data\":\"141842d3a9a\",\"visdat\":{\"141842d3a9a\":[\"function (y) \",\"x\"],\"141842046b43\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\r\nHere we see a timeline which shows all the nuclear powers in the world(as of 1998) and when they first reached this capability. In order of first detonation, these countries are the USA, USSR, UK, France, China, India and Pakistan.\r\n2. Which country has detonated the most nuclear weapons?\r\n\r\n\r\n{\"x\":{\"data\":[{\"orientation\":\"v\",\"width\":45,\"base\":2.55,\"x\":[22.5],\"y\":[0.9],\"text\":\"n:   45<br />fct_reorder(country, n): CHINA<br />country: CHINA\",\"type\":\"bar\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(248,118,109,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"transparent\"}},\"name\":\"CHINA\",\"legendgroup\":\"CHINA\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"orientation\":\"v\",\"width\":210,\"base\":4.55,\"x\":[105],\"y\":[0.9],\"text\":\"n:  210<br />fct_reorder(country, n): FRANCE<br />country: FRANCE\",\"type\":\"bar\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(196,154,0,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"transparent\"}},\"name\":\"FRANCE\",\"legendgroup\":\"FRANCE\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"orientation\":\"v\",\"width\":3,\"base\":1.55,\"x\":[1.5],\"y\":[0.9],\"text\":\"n:    3<br />fct_reorder(country, n): INDIA<br />country: INDIA\",\"type\":\"bar\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(83,180,0,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"transparent\"}},\"name\":\"INDIA\",\"legendgroup\":\"INDIA\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"orientation\":\"v\",\"width\":2,\"base\":0.55,\"x\":[1],\"y\":[0.9],\"text\":\"n:    2<br />fct_reorder(country, n): PAKIST<br />country: PAKIST\",\"type\":\"bar\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,192,148,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"transparent\"}},\"name\":\"PAKIST\",\"legendgroup\":\"PAKIST\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"orientation\":\"v\",\"width\":45,\"base\":3.55,\"x\":[22.5],\"y\":[0.9],\"text\":\"n:   45<br />fct_reorder(country, n): UK<br />country: UK\",\"type\":\"bar\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,182,235,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"transparent\"}},\"name\":\"UK\",\"legendgroup\":\"UK\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"orientation\":\"v\",\"width\":1032,\"base\":6.55,\"x\":[516],\"y\":[0.9],\"text\":\"n: 1032<br />fct_reorder(country, n): USA<br />country: USA\",\"type\":\"bar\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(165,138,255,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"transparent\"}},\"name\":\"USA\",\"legendgroup\":\"USA\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"orientation\":\"v\",\"width\":714,\"base\":5.55,\"x\":[357],\"y\":[0.9],\"text\":\"n:  714<br />fct_reorder(country, n): USSR<br />country: USSR\",\"type\":\"bar\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(251,97,215,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"transparent\"}},\"name\":\"USSR\",\"legendgroup\":\"USSR\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":45.4063926940639,\"r\":7.30593607305936,\"b\":27.2146118721461,\"l\":46.027397260274},\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"title\":{\"text\":\"Total nuclear bomb detonations\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":17.5342465753425},\"x\":0,\"xref\":\"paper\"},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-51.6,1083.6],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"250\",\"500\",\"750\",\"1000\"],\"tickvals\":[0,250,500,750,1000],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"250\",\"500\",\"750\",\"1000\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":3.65296803652968,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[0.4,7.6],\"tickmode\":\"array\",\"ticktext\":[\"PAKIST\",\"INDIA\",\"CHINA\",\"UK\",\"FRANCE\",\"USSR\",\"USA\"],\"tickvals\":[1,2,3,4,5,6,7],\"categoryorder\":\"array\",\"categoryarray\":[\"PAKIST\",\"INDIA\",\"CHINA\",\"UK\",\"FRANCE\",\"USSR\",\"USA\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":3.65296803652968,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":null,\"bordercolor\":null,\"borderwidth\":0,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"14181bde4d58\":{\"x\":{},\"y\":{},\"fill\":{},\"type\":\"bar\"}},\"cur_data\":\"14181bde4d58\",\"visdat\":{\"14181bde4d58\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\r\nThe United States and the USSR comprise a large majority of the total nuclear detonations made.\r\n3. How have these detonation patterns varied over time - what effect did the fall of the Soviet Union have on this?\r\n\r\n\r\n{\"x\":{\"data\":[{\"orientation\":\"v\",\"width\":[0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091,0.900000000000091],\"base\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"x\":[1945,1946,1948,1949,1951,1952,1953,1954,1955,1956,1957,1958,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1998],\"y\":[3,2,3,1,18,11,18,16,24,33,55,116,3,71,178,50,60,58,76,64,79,67,64,53,56,48,55,44,51,54,66,58,54,50,49,55,57,36,23,47,40,28,18,14,8,1,2,7,3,4],\"text\":[\"year: 1945<br />n:   3\",\"year: 1946<br />n:   2\",\"year: 1948<br />n:   3\",\"year: 1949<br />n:   1\",\"year: 1951<br />n:  18\",\"year: 1952<br />n:  11\",\"year: 1953<br />n:  18\",\"year: 1954<br />n:  16\",\"year: 1955<br />n:  24\",\"year: 1956<br />n:  33\",\"year: 1957<br />n:  55\",\"year: 1958<br />n: 116\",\"year: 1960<br />n:   3\",\"year: 1961<br />n:  71\",\"year: 1962<br />n: 178\",\"year: 1963<br />n:  50\",\"year: 1964<br />n:  60\",\"year: 1965<br />n:  58\",\"year: 1966<br />n:  76\",\"year: 1967<br />n:  64\",\"year: 1968<br />n:  79\",\"year: 1969<br />n:  67\",\"year: 1970<br />n:  64\",\"year: 1971<br />n:  53\",\"year: 1972<br />n:  56\",\"year: 1973<br />n:  48\",\"year: 1974<br />n:  55\",\"year: 1975<br />n:  44\",\"year: 1976<br />n:  51\",\"year: 1977<br />n:  54\",\"year: 1978<br />n:  66\",\"year: 1979<br />n:  58\",\"year: 1980<br />n:  54\",\"year: 1981<br />n:  50\",\"year: 1982<br />n:  49\",\"year: 1983<br />n:  55\",\"year: 1984<br />n:  57\",\"year: 1985<br />n:  36\",\"year: 1986<br />n:  23\",\"year: 1987<br />n:  47\",\"year: 1988<br />n:  40\",\"year: 1989<br />n:  28\",\"year: 1990<br />n:  18\",\"year: 1991<br />n:  14\",\"year: 1992<br />n:   8\",\"year: 1993<br />n:   1\",\"year: 1994<br />n:   2\",\"year: 1995<br />n:   7\",\"year: 1996<br />n:   3\",\"year: 1998<br />n:   4\"],\"type\":\"bar\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(160,32,240,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"transparent\"}},\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":45.4063926940639,\"r\":7.30593607305936,\"b\":41.8264840182648,\"l\":43.1050228310502},\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"title\":{\"text\":\"Total number of nuclear explosions by year\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":17.5342465753425},\"x\":0,\"xref\":\"paper\"},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[1941.855,2001.145],\"tickmode\":\"array\",\"ticktext\":[\"1945\",\"1950\",\"1960\",\"1970\",\"1980\",\"1990\"],\"tickvals\":[1945,1950,1960,1970,1980,1990],\"categoryorder\":\"array\",\"categoryarray\":[\"1945\",\"1950\",\"1960\",\"1970\",\"1980\",\"1990\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":3.65296803652968,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"year\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-8.9,186.9],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"50\",\"100\",\"150\"],\"tickvals\":[0,50,100,150],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"50\",\"100\",\"150\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":3.65296803652968,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"n\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":null,\"bordercolor\":null,\"borderwidth\":0,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"141811bf57a2\":{\"x\":{},\"y\":{},\"type\":\"bar\"}},\"cur_data\":\"141811bf57a2\",\"visdat\":{\"141811bf57a2\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\r\nThis is a very interesting distribution. We can see how the first weapons were developed in the mid 1940’s after which the testing heavily accelerated in the 1950’s before coming to an abrupt stop in 1959 (apparently due to a nuclear testing moratorium between the US, USSR and the UK from November 1958 to August 1961). However testing increased back up and remained at a consistently high level during the 2 decades from 1960 - 1980 which also corresponded to the peak of the cold war.\r\n1962 in particular was a crazy year with a total of 178 detonations performed - a nuclear explosion every other day. It is also interesting that this was the year where the world saw the Cuban Missile Crisis in which the tensions between the USA and the USSR peaked and the world appeared to be at the edge of a nuclear calamity.\r\nIn the mid to late 1980’s, testing dropped down significantly and plummeted furthermore in the 1990’s until there were only a couple of tests each year. The Soviet Union dissolved between 1988-1991 and it is interesting to see how the nuclear testing in the world fell sharply during and after this period.\r\n4. Where were these tested by each country? And how did this look over time?\r\n\r\n\r\n\r\nThe US appears to have tested in the mainland(mostly in Nevada) and in the Pacific over water and over some Island and some tests in the South Athlantic. The only 2 bombs used in combat ever can be observed over Japan. USSR tested all over their mainland, concentrated in small areas in the south in what appears to be modern day Kazakhstan, Uzbekistan and other former Soviet terriories and in the Novaya Zemlaya islands in the north. UK never tested on mainland, instead tested in Australia, off the coast of Australia and interstingly in mainland America(Nevada). France never tested in their mainland, but instead in what appears to be Algeria and in the South Pacific. China, India and Pakistan appear to have tested in their respective mainlands exclusively.\r\nHere we have a small animation displaying these detonations occuring over time around the world.\r\n\r\n\r\n\r\n5. Which countries definitevely have a fusion bomb(2nd generation of nukes) and what are the yields of the most destructive bombs for each country?\r\nFusion bombs are the 2nd generation of nuclear bombs - they use a fission bomb to activate a fusion reaction which releases a vast amount of energy. The destructive capabilities of fusion bombs can be several order of magnitudes higher than a regular fission bomb. Anything above 500kT yield is definitely a thermonuclear bomb(fusion bomb as opposed to a fission bomb) so here I explored which countries definitively have this capability (as of 1998).\r\n\r\n\r\n\r\nWe can observe at least 100 large fusion bomb tests in total. These have primarily undertaken by the USA and USSR but China, UK and France have definitely shown capability too (as of 1998) .\r\nThen we explore the yield of the strongest bombs produced by each country\r\n\r\n\r\n{\"x\":{\"data\":[{\"x\":[5],\"y\":[8000],\"text\":\"fct_reorder(country, yield_upper): CHINA<br />yield_upper:  8000<br />country: CHINA\",\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(248,118,109,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(248,118,109,1)\"}},\"hoveron\":\"points\",\"name\":\"CHINA\",\"legendgroup\":\"CHINA\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[3],\"y\":[1000],\"text\":\"fct_reorder(country, yield_upper): FRANCE<br />yield_upper:  1000<br />country: FRANCE\",\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(196,154,0,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(196,154,0,1)\"}},\"hoveron\":\"points\",\"name\":\"FRANCE\",\"legendgroup\":\"FRANCE\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1],\"y\":[20],\"text\":\"fct_reorder(country, yield_upper): INDIA<br />yield_upper:    20<br />country: INDIA\",\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(83,180,0,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(83,180,0,1)\"}},\"hoveron\":\"points\",\"name\":\"INDIA\",\"legendgroup\":\"INDIA\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[2],\"y\":[35],\"text\":\"fct_reorder(country, yield_upper): PAKIST<br />yield_upper:    35<br />country: PAKIST\",\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,192,148,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,192,148,1)\"}},\"hoveron\":\"points\",\"name\":\"PAKIST\",\"legendgroup\":\"PAKIST\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[4],\"y\":[3000],\"text\":\"fct_reorder(country, yield_upper): UK<br />yield_upper:  3000<br />country: UK\",\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,182,235,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,182,235,1)\"}},\"hoveron\":\"points\",\"name\":\"UK\",\"legendgroup\":\"UK\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[6],\"y\":[15000],\"text\":\"fct_reorder(country, yield_upper): USA<br />yield_upper: 15000<br />country: USA\",\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(165,138,255,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(165,138,255,1)\"}},\"hoveron\":\"points\",\"name\":\"USA\",\"legendgroup\":\"USA\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[7],\"y\":[50000],\"text\":\"fct_reorder(country, yield_upper): USSR<br />yield_upper: 50000<br />country: USSR\",\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(251,97,215,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(251,97,215,1)\"}},\"hoveron\":\"points\",\"name\":\"USSR\",\"legendgroup\":\"USSR\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":45.4063926940639,\"r\":7.30593607305936,\"b\":27.2146118721461,\"l\":54.7945205479452},\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"title\":{\"text\":\"Most Powerful Bombs by Country\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":17.5342465753425},\"x\":0,\"xref\":\"paper\"},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[0.4,7.6],\"tickmode\":\"array\",\"ticktext\":[\"INDIA\",\"PAKIST\",\"FRANCE\",\"UK\",\"CHINA\",\"USA\",\"USSR\"],\"tickvals\":[1,2,3,4,5,6,7],\"categoryorder\":\"array\",\"categoryarray\":[\"INDIA\",\"PAKIST\",\"FRANCE\",\"UK\",\"CHINA\",\"USA\",\"USSR\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":3.65296803652968,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-2479,52499],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"10000\",\"20000\",\"30000\",\"40000\",\"50000\"],\"tickvals\":[0,10000,20000,30000,40000,50000],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"10000\",\"20000\",\"30000\",\"40000\",\"50000\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":3.65296803652968,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\" Max Yield (kT of TNT)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":null,\"bordercolor\":null,\"borderwidth\":0,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"1418cbc5a3a\":{\"x\":{},\"y\":{},\"colour\":{},\"type\":\"scatter\"}},\"cur_data\":\"1418cbc5a3a\",\"visdat\":{\"1418cbc5a3a\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\r\n6. How did methods of testing them change over time?\r\nWe focus on the 5 most common methods of testing out of a total 20. These 5 were: AIRDROP, ATMOSPH, SHAFT, SHAFT/GR, TUNNEL. AIRDROP is a device dropped from an aircraft and exploded in the atmosphere and ATMOSPH is also a method of testing performed above ground/water. SHAFT, SHAFT/GR, and TUNNEL are all underground methods of testing with their own different procedures.The results in how common they were over time are as follows.\r\n\r\n\r\n{\"x\":{\"data\":[{\"x\":[1945,1946,1951,1952,1953,1955,1956,1957,1958,1962,1965,1966,1967,1968,1969,1970,1972,1973,1974,1976],\"y\":[2,1,10,5,3,3,3,5,3,29,1,2,2,1,1,1,2,2,1,1],\"text\":[\"year: 1945<br />n:  2<br />type: AIRDROP\",\"year: 1946<br />n:  1<br />type: AIRDROP\",\"year: 1951<br />n: 10<br />type: AIRDROP\",\"year: 1952<br />n:  5<br />type: AIRDROP\",\"year: 1953<br />n:  3<br />type: AIRDROP\",\"year: 1955<br />n:  3<br />type: AIRDROP\",\"year: 1956<br />n:  3<br />type: AIRDROP\",\"year: 1957<br />n:  5<br />type: AIRDROP\",\"year: 1958<br />n:  3<br />type: AIRDROP\",\"year: 1962<br />n: 29<br />type: AIRDROP\",\"year: 1965<br />n:  1<br />type: AIRDROP\",\"year: 1966<br />n:  2<br />type: AIRDROP\",\"year: 1967<br />n:  2<br />type: AIRDROP\",\"year: 1968<br />n:  1<br />type: AIRDROP\",\"year: 1969<br />n:  1<br />type: AIRDROP\",\"year: 1970<br />n:  1<br />type: AIRDROP\",\"year: 1972<br />n:  2<br />type: AIRDROP\",\"year: 1973<br />n:  2<br />type: AIRDROP\",\"year: 1974<br />n:  1<br />type: AIRDROP\",\"year: 1976<br />n:  1<br />type: AIRDROP\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(68,1,84,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"AIRDROP\",\"legendgroup\":\"AIRDROP\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1953,1954,1955,1956,1957,1958,1961,1962,1974,1976,1977,1978,1979,1980],\"y\":[5,7,2,5,13,34,48,63,1,2,1,2,1,1],\"text\":[\"year: 1953<br />n:  5<br />type: ATMOSPH\",\"year: 1954<br />n:  7<br />type: ATMOSPH\",\"year: 1955<br />n:  2<br />type: ATMOSPH\",\"year: 1956<br />n:  5<br />type: ATMOSPH\",\"year: 1957<br />n: 13<br />type: ATMOSPH\",\"year: 1958<br />n: 34<br />type: ATMOSPH\",\"year: 1961<br />n: 48<br />type: ATMOSPH\",\"year: 1962<br />n: 63<br />type: ATMOSPH\",\"year: 1974<br />n:  1<br />type: ATMOSPH\",\"year: 1976<br />n:  2<br />type: ATMOSPH\",\"year: 1977<br />n:  1<br />type: ATMOSPH\",\"year: 1978<br />n:  2<br />type: ATMOSPH\",\"year: 1979<br />n:  1<br />type: ATMOSPH\",\"year: 1980<br />n:  1<br />type: ATMOSPH\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(59,82,139,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"ATMOSPH\",\"legendgroup\":\"ATMOSPH\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1957,1958,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992],\"y\":[3,6,7,52,42,47,40,49,44,55,50,40,34,38,34,35,30,31,33,34,34,29,29,30,30,39,22,14,26,23,16,8,7,4],\"text\":[\"year: 1957<br />n:  3<br />type: SHAFT\",\"year: 1958<br />n:  6<br />type: SHAFT\",\"year: 1961<br />n:  7<br />type: SHAFT\",\"year: 1962<br />n: 52<br />type: SHAFT\",\"year: 1963<br />n: 42<br />type: SHAFT\",\"year: 1964<br />n: 47<br />type: SHAFT\",\"year: 1965<br />n: 40<br />type: SHAFT\",\"year: 1966<br />n: 49<br />type: SHAFT\",\"year: 1967<br />n: 44<br />type: SHAFT\",\"year: 1968<br />n: 55<br />type: SHAFT\",\"year: 1969<br />n: 50<br />type: SHAFT\",\"year: 1970<br />n: 40<br />type: SHAFT\",\"year: 1971<br />n: 34<br />type: SHAFT\",\"year: 1972<br />n: 38<br />type: SHAFT\",\"year: 1973<br />n: 34<br />type: SHAFT\",\"year: 1974<br />n: 35<br />type: SHAFT\",\"year: 1975<br />n: 30<br />type: SHAFT\",\"year: 1976<br />n: 31<br />type: SHAFT\",\"year: 1977<br />n: 33<br />type: SHAFT\",\"year: 1978<br />n: 34<br />type: SHAFT\",\"year: 1979<br />n: 34<br />type: SHAFT\",\"year: 1980<br />n: 29<br />type: SHAFT\",\"year: 1981<br />n: 29<br />type: SHAFT\",\"year: 1982<br />n: 30<br />type: SHAFT\",\"year: 1983<br />n: 30<br />type: SHAFT\",\"year: 1984<br />n: 39<br />type: SHAFT\",\"year: 1985<br />n: 22<br />type: SHAFT\",\"year: 1986<br />n: 14<br />type: SHAFT\",\"year: 1987<br />n: 26<br />type: SHAFT\",\"year: 1988<br />n: 23<br />type: SHAFT\",\"year: 1989<br />n: 16<br />type: SHAFT\",\"year: 1990<br />n:  8<br />type: SHAFT\",\"year: 1991<br />n:  7<br />type: SHAFT\",\"year: 1992<br />n:  4<br />type: SHAFT\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(33,144,140,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"SHAFT\",\"legendgroup\":\"SHAFT\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1989],\"y\":[2,5,9,11,10,12,10,8,4,4,4,5,1],\"text\":[\"year: 1975<br />n:  2<br />type: SHAFT/GR\",\"year: 1976<br />n:  5<br />type: SHAFT/GR\",\"year: 1977<br />n:  9<br />type: SHAFT/GR\",\"year: 1978<br />n: 11<br />type: SHAFT/GR\",\"year: 1979<br />n: 10<br />type: SHAFT/GR\",\"year: 1980<br />n: 12<br />type: SHAFT/GR\",\"year: 1981<br />n: 10<br />type: SHAFT/GR\",\"year: 1982<br />n:  8<br />type: SHAFT/GR\",\"year: 1983<br />n:  4<br />type: SHAFT/GR\",\"year: 1984<br />n:  4<br />type: SHAFT/GR\",\"year: 1985<br />n:  4<br />type: SHAFT/GR\",\"year: 1986<br />n:  5<br />type: SHAFT/GR\",\"year: 1989<br />n:  1<br />type: SHAFT/GR\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(93,200,99,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"SHAFT/GR\",\"legendgroup\":\"SHAFT/GR\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1957,1958,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992],\"y\":[2,9,4,5,1,9,12,17,15,15,15,15,13,12,7,10,11,11,11,18,12,12,9,8,14,8,6,1,12,8,3,2,1,2],\"text\":[\"year: 1957<br />n:  2<br />type: TUNNEL\",\"year: 1958<br />n:  9<br />type: TUNNEL\",\"year: 1961<br />n:  4<br />type: TUNNEL\",\"year: 1962<br />n:  5<br />type: TUNNEL\",\"year: 1963<br />n:  1<br />type: TUNNEL\",\"year: 1964<br />n:  9<br />type: TUNNEL\",\"year: 1965<br />n: 12<br />type: TUNNEL\",\"year: 1966<br />n: 17<br />type: TUNNEL\",\"year: 1967<br />n: 15<br />type: TUNNEL\",\"year: 1968<br />n: 15<br />type: TUNNEL\",\"year: 1969<br />n: 15<br />type: TUNNEL\",\"year: 1970<br />n: 15<br />type: TUNNEL\",\"year: 1971<br />n: 13<br />type: TUNNEL\",\"year: 1972<br />n: 12<br />type: TUNNEL\",\"year: 1973<br />n:  7<br />type: TUNNEL\",\"year: 1974<br />n: 10<br />type: TUNNEL\",\"year: 1975<br />n: 11<br />type: TUNNEL\",\"year: 1976<br />n: 11<br />type: TUNNEL\",\"year: 1977<br />n: 11<br />type: TUNNEL\",\"year: 1978<br />n: 18<br />type: TUNNEL\",\"year: 1979<br />n: 12<br />type: TUNNEL\",\"year: 1980<br />n: 12<br />type: TUNNEL\",\"year: 1981<br />n:  9<br />type: TUNNEL\",\"year: 1982<br />n:  8<br />type: TUNNEL\",\"year: 1983<br />n: 14<br />type: TUNNEL\",\"year: 1984<br />n:  8<br />type: TUNNEL\",\"year: 1985<br />n:  6<br />type: TUNNEL\",\"year: 1986<br />n:  1<br />type: TUNNEL\",\"year: 1987<br />n: 12<br />type: TUNNEL\",\"year: 1988<br />n:  8<br />type: TUNNEL\",\"year: 1989<br />n:  3<br />type: TUNNEL\",\"year: 1990<br />n:  2<br />type: TUNNEL\",\"year: 1991<br />n:  1<br />type: TUNNEL\",\"year: 1992<br />n:  2<br />type: TUNNEL\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(253,231,37,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"TUNNEL\",\"legendgroup\":\"TUNNEL\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":45.4063926940639,\"r\":7.30593607305936,\"b\":41.8264840182648,\"l\":37.2602739726027},\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"title\":{\"text\":\"Most common nuclear bomb testing methods over time\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":17.5342465753425},\"x\":0,\"xref\":\"paper\"},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[1942.65,1994.35],\"tickmode\":\"array\",\"ticktext\":[\"1950\",\"1960\",\"1970\",\"1980\",\"1990\"],\"tickvals\":[1950,1960,1970,1980,1990],\"categoryorder\":\"array\",\"categoryarray\":[\"1950\",\"1960\",\"1970\",\"1980\",\"1990\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":3.65296803652968,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"year\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-2.1,66.1],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"20\",\"40\",\"60\"],\"tickvals\":[0,20,40,60],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"20\",\"40\",\"60\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":3.65296803652968,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"n\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":true,\"legend\":{\"bgcolor\":null,\"bordercolor\":null,\"borderwidth\":0,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895},\"y\":1},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"14182a7522a9\":{\"x\":{},\"y\":{},\"colour\":{},\"type\":\"scatter\"}},\"cur_data\":\"14182a7522a9\",\"visdat\":{\"14182a7522a9\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\r\nIt is interesting to see that in the earlier days of testing ATMOSPH and AIRDROP - which are the two above ground/water methods of testing were the most popular. However, in the 1960’s and 1970’s these methods of testing plummeted and methods of testing that were underground became more common (sidenote: this was apparently to minimize any health hazards from radioactive dust and waste being blown far by the wind into populated areas.)\r\n7. Very simple application that lets you compare each countries nuclear detonation activity over time.\r\nLink to app.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-04-nuclearexplosions/nuclearexplosions_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-04-04T00:33:40-05:00",
    "input_file": {}
  }
]
