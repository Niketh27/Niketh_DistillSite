---
title: "Lending Club Model"
description: |
  A Loan Payback Prediction Model with lasso, random forest and knn.
author:
  - name: Niketh Gamage
    url: {}
date: 04-06-2021
output:
  distill::distill_article:
    self_contained: false
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

```{r libraries}
# SEE modeldata package for new datasets
library(tidyverse)         # for graphing and data cleaning
library(tidymodels)        # for modeling
library(stacks)            # for stacking models
library(naniar)            # for examining missing values (NAs)
library(lubridate)         # for date manipulation
library(moderndive)        # for King County housing data
library(vip)               # for variable importance plots
library(DALEX)             # for model interpretation  
library(DALEXtra)          # for extension of DALEX
library(patchwork)         # for combining plots nicely
theme_set(theme_minimal()) # for theming

library(ranger) # for random forest model
library(kknn)   #for knn model
library(ROSE)  #for ROC curve
```

```{r data}
data("lending_club")
```

      

## Modeling

We will be using the lending_club dataset, which is available in the 'modeldata' library in the 'tidymodels' package, to predict the variable Class which shows if the loan is fully paid back/being paid back (category: good) or if it is defaulted/late(category: bad). 

We will try to build 3 types of models - lasso, random forest and knn - and make a final optimized model that combines all of this information. 

Some data exploration and checking the distributions of the variables. 

```{r}
lending_club 
```


```{r}
# numeric variables

lending_club %>% 
  select(where(is.numeric)) %>% 
  pivot_longer(cols = everything(),
               names_to = "variable", 
               values_to = "value") %>% 
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30) +
  facet_wrap(vars(variable), 
             scales = "free")
```

```{r}
# categorical variables

lending_club %>% 
  select(where(is.factor)) %>% 
  pivot_longer(cols = everything(),
               names_to = "variable", 
               values_to = "value") %>% 
  ggplot(aes(x = value)) +
  geom_bar() +
  facet_wrap(vars(variable), 
             scales = "free", 
             nrow = 2)
```

We do some data cleaning steps to get rid of variables that are redundant/ identical to the chosen response variable 'Class'.

```{r}
#get rid of zero or near zero variance variables
lending_club2 <- lending_club %>% 
  na.omit() %>% 
  select(-delinq_amnt) %>% 
  select(-acc_now_delinq)

lending_club
lending_club2


```

We saw earlier from the distribution of `Class` that there is a lot more `good` rows than `bad`. So, we resample with replacement some `bad` cases and add them to our dataset so that the `good` cases don't overwhelm our classification model into predicting `good` all the time. 

```{r}
create_more_bad <- lending_club2 %>% 
  filter(Class == "bad") %>% 
  sample_n(size = 3000, replace = TRUE)

lending_club_mod <- lending_club2 %>% 
  bind_rows(create_more_bad)

```

Then we split the data into training and testing set.

```{r}
set.seed(494) # for reproducibility

lending_split <- initial_split(lending_club_mod, prop = 0.75)

lending_training <- training(lending_split)
lending_testing <- testing(lending_split)

```

Then we create the recipe and pre-process the data to make it ready to build our first model: lasso. 

```{r}

lending_recipe <- recipe(Class ~ . , data = lending_training) %>% 
  # making all integer variables are numeric
  step_mutate_at(all_numeric(), fn = ~as.numeric(.)) %>% 
  
   # making categorical variables dummy variables
  step_dummy(all_nominal(),-all_outcomes()) %>% 

  #quantitative variables are normalized
  step_normalize(all_predictors(), 
                 -all_nominal(),
                 -has_role(match = 'evaluative'))  
  
 
```

Checking if everything looks ok and normalized: 

```{r}
lending_recipe %>% 
  prep(lending_training) %>% 
  juice()
```

Then we set up the lasso model and workflow and set the penalty parameter to tune().

```{r}
#define lasso model
lending_lasso_mod <- 
  logistic_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet") %>% 
  set_mode("classification")
```

```{r}
# create workflow
lending_lasso_wf <- 
  workflow() %>% 
  add_recipe(lending_recipe) %>% 
  add_model(lending_lasso_mod)
```

6. Set up the model tuning for the `penalty` parameter. Be sure to add the `control_stack_grid()` for the `control` argument so we can use these results later when we stack. Find the accuracy and area under the roc curve for the model with the best tuning parameter.  Use 5-fold cv.

Setting up model tuning for the penalty parameter with 5-fold cross validation using the training dataset

```{r}
set.seed(494) #for reproducible 5-fold

lending_cv <- vfold_cv(lending_training, v = 5)

penalty_grid <- grid_regular(penalty(),
                             levels = 10)

accuracy_met <- metric_set(accuracy)

# tune the model 
lending_lasso_tune <- 
  lending_lasso_wf %>% 
  tune_grid(
    resamples = lending_cv,
    grid = penalty_grid,
    control = control_stack_grid()
#    metrics = accuracy_met
    )


```

Finding the best tuning parameter and finalizing the model.
```{r}
# Best tuning parameter by smallest rmse
best_param <- lending_lasso_tune %>% 
  select_best(metric = "accuracy")

lending_lasso_final_wf <- lending_lasso_wf %>% 
  finalize_workflow(best_param)

lending_lasso_final_mod <- lending_lasso_final_wf %>% 
  fit(data = lending_training)


```

Let's take a look at the model estimates for our predictors
```{r}
lending_lasso_final_mod %>% 
  pull_workflow_fit() %>% 
  tidy() 
```


Then we fit the model with the testing data and check the metrics.
```{r}
# Fit model with best tuning parameter(s) to training data and apply to test data
lending_lasso_test <- lending_lasso_final_wf %>% 
  last_fit(lending_split)

# Metrics for model applied to test data
lending_lasso_test %>% 
  collect_metrics()
```

Not amazing but not too bad either. Now let's move on to the random forest model.


Setting up the recipe and the pre-processing steps to build a random forest model.  

```{r}
# set up recipe and transformation steps and roles
lendingrf_recipe <- 
  recipe(formula = Class ~ ., 
         data = lending_training) %>% 
  step_mutate_at(all_numeric(), 
            fn= ~as.numeric(.)) 

```


Setting up the random forest model and workflow and we tune over min_n and mtry

```{r}
#define model
lendingrf_spec <- 
  rand_forest(mtry = tune(), 
              min_n = tune(), 
              trees = 100) %>% 
  set_mode("classification") %>% 
  set_engine("ranger")

#create workflow
lendingrf_wf <- 
  workflow() %>% 
  add_recipe(lendingrf_recipe) %>% 
  add_model(lendingrf_spec) 
```


Tuning the model using 5 fold cross-validation

```{r}
#fit the model
set.seed(494) # for reproducibility - random sampling in random forest choosing number of variables


rfpenalty_grid <- grid_regular(finalize(mtry(), lending_training %>% select(-Class)), min_n(), levels = 3)



lendingrf_tune <-
  lendingrf_wf %>% 
  tune_grid(
    resamples = lending_cv,
    control = control_stack_grid(),
    grid = rfpenalty_grid)


```


Finalizing the model and fitting it to the testing data
```{r}

# Best tuning parameter by smallest rmse
bestrf_param <- lendingrf_tune %>% 
  select_best(metric = "accuracy")

lendingrf_final_wf <- lendingrf_wf %>% 
  finalize_workflow(bestrf_param)


lendingrf_final_mod<- lendingrf_final_wf %>% 
  fit(lending_training)
  
lendingrf_last_fit <- lendingrf_final_wf %>% 
  last_fit(lending_split) 




```

Metrics for the finalized random forest model

```{r}

lendingrf_last_fit%>% 
  collect_metrics()
```


Then we use the DALEX and DALEXtra libraries to build plots of the residuals of each of the models.

```{r}

lasso_explain <-
  explain_tidymodels(
    model = lending_lasso_final_mod,
    data = lending_training %>% select(-Class),
    y = as.numeric(lending_training %>%  pull(Class)),
    label = "lasso"
  )


rf_explain <- 
  explain_tidymodels(
    model = lendingrf_final_mod,
    data = lending_training %>% select(-Class), 
    y = as.numeric(lending_training %>%  pull(Class)),
    label = "rf"
  )
```
```{r}
lasso_mod_perf <- model_performance(lasso_explain)
rf_mod_perf <-  model_performance(rf_explain)
```


```{r}
hist_plot <- 
  plot(lasso_mod_perf,
       rf_mod_perf, 
       geom = "histogram")
box_plot <-
  plot(lasso_mod_perf,
       rf_mod_perf, 
       geom = "boxplot")

hist_plot + box_plot
```


Creating a variable importance plot to check out the most significant predictors.

```{r}

set.seed(494) #since we are sampling & permuting, we set a seed so we can replicate the results
lasso_var_imp <- 
  model_parts(
    lasso_explain
    )

plot(lasso_var_imp, show_boxplots = TRUE)

set.seed(10) #since we are sampling & permuting, we set a seed so we can replicate the results
rf_var_imp <- 
  model_parts(
    rf_explain
    )

plot(rf_var_imp, show_boxplots = TRUE)
```
Interest rate, annual income and verification status seem to be very important predictors which is not all that suprising.

Now we set up the knn model and tune k. 

```{r}
# create a model definition
knn_mod <-
  nearest_neighbor(
    neighbors = tune("k")
  ) %>%
  set_engine("kknn") %>% 
  set_mode("classification")

# create the workflow
knn_wf <- 
  workflow() %>% 
  add_model(knn_mod) %>%
  add_recipe(lending_recipe)

# tune it using 4 tuning parameters
knn_tune <- 
  knn_wf %>% 
  tune_grid(
    lending_cv,
    grid = 4,
    control = control_stack_grid()
  )
```


16. Create a model stack with the candidate models from the previous parts of the exercise and use the `blend_predictions()` function to find the coefficients of the stacked model. Create a plot examining the performance metrics for the different penalty parameters to assure you have captured the best one. If not, adjust the penalty. (HINT: use the `autoplot()` function). Which models are contributing most?

Now we stack the 3 model tuning parameters and 'blend' them to optimize our final model.

```{r}
lending_stack <- 
  stacks() %>% 
   add_candidates(lendingrf_tune) %>% 
   add_candidates(lending_lasso_tune) %>% 
   add_candidates(knn_tune)

as_tibble(lending_stack)
```


```{r}
lending_blend <- 
  lending_stack %>% 
  blend_predictions() 

```

```{r}
autoplot(lending_blend)
```

```{r}
lending_blend
```



We then create the model stack and apply it onto our test data to see how it performs. 

```{r}
lending_final_stack <- lending_blend %>% 
  fit_members()
```

```{r}
  lending_final_stack %>% 
  predict(new_data = lending_testing) %>% 
  bind_cols(lending_testing)
```

```{r}
pred_compare <- lending_final_stack %>% 
  predict(new_data = lending_testing) %>% 
  bind_cols(lending_testing) %>% 
  select(.pred_class,Class) 
```

```{r}
predictions <- pred_compare %>% 
  pull(.pred_class)
true_class <- pred_compare %>% 
  pull(Class)
```

```{r}
roc.curve(true_class, predictions)
```
We get an auc_roc of 0.95 which is very very good for a classification model of this nature.

## Shiny app

Work In Progress.


